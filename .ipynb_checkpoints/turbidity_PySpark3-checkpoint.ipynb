{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "crlKd6Qpq6BU",
    "outputId": "595b1bab-4f91-4c19-9eeb-5dfb9f4b5ecd"
   },
   "outputs": [],
   "source": [
    "# Import findspark and initialize. \n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "t3vOTJF5rD_4"
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType,StructField,StringType, DateType,IntegerType\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-mOHq6oaizia"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-------------------+--------------+----------------+----------------+------------------+------------+--------------+-----------+-------------+-------------+---------------+----------+------------+------------+--------------+---------------+\n",
      "|  _c0|      Date|               Time|LKSPOMET_ATemp|LKSPOMET_F_ATemp|LKSPOMET_TotPrcp|LKSPOMET_F_TotPrcp|LKSBAWQ_Temp|LKSBAWQ_F_Temp|LKSBAWQ_Sal|LKSBAWQ_F_Sal|LKSBAWQ_Depth|LKSBAWQ_F_Depth|LKSBAWQ_pH|LKSBAWQ_F_pH|LKSBAWQ_Turb|LKSBAWQ_F_Turb|Turbidity_Range|\n",
      "+-----+----------+-------------------+--------------+----------------+----------------+------------------+------------+--------------+-----------+-------------+-------------+---------------+----------+------------+------------+--------------+---------------+\n",
      "|14454|05/31/2018|2023-06-04 13:00:00|          23.9|            <0> |             0.0|              <0> |        17.2|          <0> |        0.1|         <0> |         1.49|           <0> |       7.8|        <0> |        15.0|          <0> |          </=15|\n",
      "|14455|05/31/2018|2023-06-04 13:15:00|          24.0|            <0> |             0.0|              <0> |        17.2|          <0> |        0.1|         <0> |         1.51|           <0> |       7.8|        <0> |        15.0|          <0> |          </=15|\n",
      "|14456|05/31/2018|2023-06-04 13:30:00|          23.9|            <0> |             0.0|              <0> |        17.3|          <0> |        0.1|         <0> |         1.53|           <0> |       7.8|        <0> |        15.0|          <0> |          </=15|\n",
      "|14457|05/31/2018|2023-06-04 13:45:00|          24.2|            <0> |             0.0|              <0> |        17.4|          <0> |        0.1|         <0> |         1.55|           <0> |       7.8|        <0> |        14.0|          <0> |          </=15|\n",
      "|14458|05/31/2018|2023-06-04 14:00:00|          24.4|            <0> |             0.0|              <0> |        17.4|          <0> |        0.1|         <0> |         1.58|           <0> |       7.8|        <0> |        15.0|          <0> |          </=15|\n",
      "|14459|05/31/2018|2023-06-04 14:15:00|          24.4|            <0> |             0.0|              <0> |        17.6|          <0> |        0.1|         <0> |         1.58|           <0> |       7.8|        <0> |        14.0|          <0> |          </=15|\n",
      "|14460|05/31/2018|2023-06-04 14:30:00|          24.3|            <0> |             0.0|              <0> |        17.4|          <0> |        0.1|         <0> |         1.58|           <0> |       7.8|        <0> |        15.0|          <0> |          </=15|\n",
      "|14461|05/31/2018|2023-06-04 14:45:00|          24.7|            <0> |             0.0|              <0> |        17.7|          <0> |        0.1|         <0> |         1.57|           <0> |       7.9|        <0> |        14.0|          <0> |          </=15|\n",
      "|14462|05/31/2018|2023-06-04 15:00:00|          24.4|            <0> |             0.0|              <0> |        17.6|          <0> |        0.1|         <0> |         1.59|           <0> |       7.8|        <0> |        13.0|          <0> |          </=15|\n",
      "|14463|05/31/2018|2023-06-04 15:15:00|          24.4|            <0> |             0.0|              <0> |        17.8|          <0> |        0.1|         <0> |          1.6|           <0> |       7.9|        <0> |        13.0|          <0> |          </=15|\n",
      "|14464|05/31/2018|2023-06-04 15:30:00|          24.1|            <0> |             0.0|              <0> |        17.9|          <0> |        0.1|         <0> |          1.6|           <0> |       7.9|        <0> |        12.0|          <0> |          </=15|\n",
      "|14465|05/31/2018|2023-06-04 15:45:00|          24.1|            <0> |             0.0|              <0> |        17.9|          <0> |        0.1|         <0> |         1.61|           <0> |       7.9|        <0> |        13.0|          <0> |          </=15|\n",
      "|14466|05/31/2018|2023-06-04 16:00:00|          24.2|            <0> |             0.0|              <0> |        17.9|          <0> |        0.1|         <0> |         1.58|           <0> |       7.9|        <0> |        13.0|          <0> |          </=15|\n",
      "|14467|05/31/2018|2023-06-04 16:15:00|          24.6|            <0> |             0.0|              <0> |        17.9|          <0> |        0.1|         <0> |         1.62|           <0> |       7.9|        <0> |        13.0|          <0> |          </=15|\n",
      "|14468|05/31/2018|2023-06-04 16:30:00|          24.7|            <0> |             0.0|              <0> |        18.1|          <0> |        0.1|         <0> |          1.6|           <0> |       7.9|        <0> |        14.0|          <0> |          </=15|\n",
      "|14469|05/31/2018|2023-06-04 16:45:00|          24.8|            <0> |             0.0|              <0> |        18.1|          <0> |        0.1|         <0> |         1.58|           <0> |       7.9|        <0> |        11.0|          <0> |          </=15|\n",
      "|14470|05/31/2018|2023-06-04 17:00:00|          24.7|            <0> |             0.0|              <0> |        18.0|          <0> |        0.1|         <0> |          1.6|           <0> |       7.8|        <0> |        12.0|          <0> |          </=15|\n",
      "|14471|05/31/2018|2023-06-04 17:15:00|          24.4|            <0> |             0.0|              <0> |        18.1|          <0> |        0.1|         <0> |         1.58|           <0> |       7.9|        <0> |        12.0|          <0> |          </=15|\n",
      "|14472|05/31/2018|2023-06-04 17:30:00|          24.2|            <0> |             0.0|              <0> |        18.1|          <0> |        0.1|         <0> |         1.54|           <0> |       7.9|        <0> |        13.0|          <0> |          </=15|\n",
      "|14473|05/31/2018|2023-06-04 17:45:00|          24.1|            <0> |             0.0|              <0> |        18.2|          <0> |        0.1|         <0> |         1.53|           <0> |       7.9|        <0> |        12.0|          <0> |          </=15|\n",
      "+-----+----------+-------------------+--------------+----------------+----------------+------------------+------------+--------------+-----------+-------------+-------------+---------------+----------+------------+------------+--------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in data from S3 Buckets\n",
    "from pyspark import SparkFiles\n",
    "url = \"turbidity_df3.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.csv(SparkFiles.get(\"turbidity_df3.csv\"), header=True, inferSchema=True)\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MZLbpHe-n_MR"
   },
   "outputs": [],
   "source": [
    "# Create our temporary view\n",
    "df.createOrReplaceTempView('turbidity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysaprk.sql.functions import split\n",
    "split_col = split(df['Date'],'/')\n",
    "df = df.withColumn('D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ednpJUL9oGTJ",
    "outputId": "1702c674-1a50-4590-e4e0-bb49239ff028"
   },
   "outputs": [],
   "source": [
    "# We can perform most any SQL action at this point\n",
    "# here we are converting the date to a more workable date object\n",
    "#NOTE: since we are not assigning this to a dataframe the change is not saved.\n",
    "spark.sql(\"\"\"SELECT show_id, \n",
    "   type, \n",
    "   title, \n",
    "   country, \n",
    "   TO_DATE(date_added, 'MMMM d, yyyy') \n",
    "   AS date_added, \n",
    "   release_year, \n",
    "   rating, \n",
    "   duration \n",
    "   FROM movies \n",
    "   WHERE date_added IS NOT null AND type='Movie'\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VVT_qRomo959",
    "outputId": "2cedab4b-e695-4f71-cd16-b294708e007c"
   },
   "outputs": [],
   "source": [
    "# All of the SQL you learned in Unit 6 is available to you in Spark SQL\n",
    "# Here we are listing out the counts by rating\n",
    "# NOTE: it is almost NEVER a good idea to \"order by\" when using Spark with large datasets (more on this in 8.2)\n",
    "spark.sql(\"\"\"\n",
    "  SELECT\n",
    "    rating,\n",
    "    count(*) AS number_of_ratings\n",
    "  FROM movies\n",
    "  GROUP BY rating\n",
    "  ORDER BY 2 DESC\n",
    "  \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1_PVtGyJzJqU"
   },
   "outputs": [],
   "source": [
    "# Let's output a file with just listing for children\n",
    "# first we will use our spark sql to write to a dataframe\n",
    "\n",
    "out_df= spark.sql(\"\"\"\n",
    "  SELECT \n",
    "  title,\n",
    "  rating,\n",
    "  date_added,\n",
    "  duration\n",
    "  FROM Movies\n",
    "  WHERE rating IN ('G','PG', 'PG-13')\"\"\")\n",
    "\n",
    "# Make sure we got what we wanted\n",
    "out_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z4csQ38M6xCl"
   },
   "outputs": [],
   "source": [
    "#  As Spark stores the data in partitions, it will also write data in partitions.\n",
    "#  These partitions will always be stored in a folder with the same name as the file, and that folder may often contain many subfolders or files.\n",
    "#  Within the partition folder, there will be a file or files that starts with `part-`, these are CSV files. \n",
    "# However, they are often not optimal for friendly reading, but can be downloaded to your computer.\n",
    "\n",
    "out_df.write.csv('movies_out_spark.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wAbO6VJL65Nb"
   },
   "outputs": [],
   "source": [
    "# The easiest work around of the part file output is to take the data to Pandas and write out a CSV.\n",
    "# This forces the data to the master node and is not recommended unless you have filtered and/or aggregated your data to a reasonable size.\n",
    "\n",
    "out_df.toPandas().to_csv('movies_out_pandas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rn9uNP137FdI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "SparkSQL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
