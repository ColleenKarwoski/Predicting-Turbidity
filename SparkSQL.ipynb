{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "crlKd6Qpq6BU",
    "outputId": "595b1bab-4f91-4c19-9eeb-5dfb9f4b5ecd"
   },
   "outputs": [],
   "source": [
    "# Import findspark and initialize. \n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "t3vOTJF5rD_4"
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType,StructField,StringType, DateType,IntegerType\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-mOHq6oaizia"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+----------------+--------------+------------+-----------+-------------+----------+------------+\n",
      "|_c0|  DateTimeStamp|LKSPOMET_TotPrcp|LKSPOMET_ATemp|LKSBAWQ_Temp|LKSBAWQ_Sal|LKSBAWQ_Depth|LKSBAWQ_pH|LKSBAWQ_Turb|\n",
      "+---+---------------+----------------+--------------+------------+-----------+-------------+----------+------------+\n",
      "|  0|01/01/2018 0:00|             0.0|         -24.5|        null|       null|         null|      null|        null|\n",
      "|  1|01/01/2018 0:15|             0.0|         -24.7|        null|       null|         null|      null|        null|\n",
      "|  2|01/01/2018 0:30|             0.0|         -24.8|        null|       null|         null|      null|        null|\n",
      "|  3|01/01/2018 0:45|             0.0|         -25.0|        null|       null|         null|      null|        null|\n",
      "|  4|01/01/2018 1:00|             0.0|         -25.2|        null|       null|         null|      null|        null|\n",
      "|  5|01/01/2018 1:15|             0.0|         -25.2|        null|       null|         null|      null|        null|\n",
      "|  6|01/01/2018 1:30|             0.0|         -25.4|        null|       null|         null|      null|        null|\n",
      "|  7|01/01/2018 1:45|             0.0|         -25.6|        null|       null|         null|      null|        null|\n",
      "|  8|01/01/2018 2:00|             0.0|         -25.8|        null|       null|         null|      null|        null|\n",
      "|  9|01/01/2018 2:15|             0.0|         -26.2|        null|       null|         null|      null|        null|\n",
      "| 10|01/01/2018 2:30|             0.0|         -26.4|        null|       null|         null|      null|        null|\n",
      "| 11|01/01/2018 2:45|             0.0|         -26.7|        null|       null|         null|      null|        null|\n",
      "| 12|01/01/2018 3:00|             0.0|         -26.5|        null|       null|         null|      null|        null|\n",
      "| 13|01/01/2018 3:15|             0.0|         -26.5|        null|       null|         null|      null|        null|\n",
      "| 14|01/01/2018 3:30|             0.0|         -26.8|        null|       null|         null|      null|        null|\n",
      "| 15|01/01/2018 3:45|             0.0|         -27.2|        null|       null|         null|      null|        null|\n",
      "| 16|01/01/2018 4:00|             0.0|         -27.1|        null|       null|         null|      null|        null|\n",
      "| 17|01/01/2018 4:15|             0.0|         -27.4|        null|       null|         null|      null|        null|\n",
      "| 18|01/01/2018 4:30|             0.0|         -27.3|        null|       null|         null|      null|        null|\n",
      "| 19|01/01/2018 4:45|             0.0|         -27.3|        null|       null|         null|      null|        null|\n",
      "+---+---------------+----------------+--------------+------------+-----------+-------------+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in data from S3 Buckets\n",
    "from pyspark import SparkFiles\n",
    "url = \"turbidity_df.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.csv(SparkFiles.get(\"turbidity_df.csv\"), header=True, inferSchema=True)\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3473735387.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[10], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    server_times = sc.parallelize([('DateTimeStamp').toDF(['ServerTime'])\u001b[0m\n\u001b[1;37m                                                                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "server_times = sc.parallelize([('DateTimeStamp').toDF(['ServerTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'ServerTime'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m unix_timestamp, from_unixtime, date_format\n\u001b[1;32m----> 3\u001b[0m df\u001b[38;5;241m.\u001b[39mselect(unix_timestamp(\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mServerTime\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm/d/yyyy h:mm\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mut\u001b[39m\u001b[38;5;124m'\u001b[39m))\\\n\u001b[0;32m      4\u001b[0m   \u001b[38;5;241m.\u001b[39mselect(from_unixtime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mut\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdty\u001b[39m\u001b[38;5;124m'\u001b[39m))\\\n\u001b[0;32m      5\u001b[0m   \u001b[38;5;241m.\u001b[39mselect(date_format(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdty\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mM/d/yyyy\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m      6\u001b[0m           date_format(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdty\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh:m:s a\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime\u001b[39m\u001b[38;5;124m'\u001b[39m))\\\n\u001b[0;32m      7\u001b[0m   \u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mC:\\spark_setup\\spark-3.3.2-bin-hadoop2\\python\\pyspark\\sql\\dataframe.py:1988\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1978\u001b[0m \u001b[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001b[39;00m\n\u001b[0;32m   1979\u001b[0m \n\u001b[0;32m   1980\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1985\u001b[0m \u001b[38;5;124;03m[Row(age=2), Row(age=5)]\u001b[39;00m\n\u001b[0;32m   1986\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m-> 1988\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1989\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name)\n\u001b[0;32m   1990\u001b[0m     )\n\u001b[0;32m   1991\u001b[0m jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mapply(name)\n\u001b[0;32m   1992\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'ServerTime'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, from_unixtime, date_format\n",
    "\n",
    "df.select(unix_timestamp(df.ServerTime, 'm/d/yyyy h:mm').alias('ut'))\\\n",
    "  .select(from_unixtime('ut').alias('dty'))\\\n",
    "  .select(date_format('dty', 'M/d/yyyy').alias('Date'),\n",
    "          date_format('dty', 'h:m:s a').alias('Time'))\\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MZLbpHe-n_MR"
   },
   "outputs": [],
   "source": [
    "# Create our temporary view\n",
    "df.createOrReplaceTempView('movies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ednpJUL9oGTJ",
    "outputId": "1702c674-1a50-4590-e4e0-bb49239ff028"
   },
   "outputs": [],
   "source": [
    "# We can perform most any SQL action at this point\n",
    "# here we are converting the date to a more workable date object\n",
    "#NOTE: since we are not assigning this to a dataframe the change is not saved.\n",
    "spark.sql(\"\"\"SELECT show_id, \n",
    "   type, \n",
    "   title, \n",
    "   country, \n",
    "   TO_DATE(date_added, 'MMMM d, yyyy') \n",
    "   AS date_added, \n",
    "   release_year, \n",
    "   rating, \n",
    "   duration \n",
    "   FROM movies \n",
    "   WHERE date_added IS NOT null AND type='Movie'\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VVT_qRomo959",
    "outputId": "2cedab4b-e695-4f71-cd16-b294708e007c"
   },
   "outputs": [],
   "source": [
    "# All of the SQL you learned in Unit 6 is available to you in Spark SQL\n",
    "# Here we are listing out the counts by rating\n",
    "# NOTE: it is almost NEVER a good idea to \"order by\" when using Spark with large datasets (more on this in 8.2)\n",
    "spark.sql(\"\"\"\n",
    "  SELECT\n",
    "    rating,\n",
    "    count(*) AS number_of_ratings\n",
    "  FROM movies\n",
    "  GROUP BY rating\n",
    "  ORDER BY 2 DESC\n",
    "  \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1_PVtGyJzJqU"
   },
   "outputs": [],
   "source": [
    "# Let's output a file with just listing for children\n",
    "# first we will use our spark sql to write to a dataframe\n",
    "\n",
    "out_df= spark.sql(\"\"\"\n",
    "  SELECT \n",
    "  title,\n",
    "  rating,\n",
    "  date_added,\n",
    "  duration\n",
    "  FROM Movies\n",
    "  WHERE rating IN ('G','PG', 'PG-13')\"\"\")\n",
    "\n",
    "# Make sure we got what we wanted\n",
    "out_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z4csQ38M6xCl"
   },
   "outputs": [],
   "source": [
    "#  As Spark stores the data in partitions, it will also write data in partitions.\n",
    "#  These partitions will always be stored in a folder with the same name as the file, and that folder may often contain many subfolders or files.\n",
    "#  Within the partition folder, there will be a file or files that starts with `part-`, these are CSV files. \n",
    "# However, they are often not optimal for friendly reading, but can be downloaded to your computer.\n",
    "\n",
    "out_df.write.csv('movies_out_spark.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wAbO6VJL65Nb"
   },
   "outputs": [],
   "source": [
    "# The easiest work around of the part file output is to take the data to Pandas and write out a CSV.\n",
    "# This forces the data to the master node and is not recommended unless you have filtered and/or aggregated your data to a reasonable size.\n",
    "\n",
    "out_df.toPandas().to_csv('movies_out_pandas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rn9uNP137FdI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "SparkSQL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
